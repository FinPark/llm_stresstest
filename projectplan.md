# LLM Stresstest - Projektplan

## Projektziel
Entwicklung einer robusten Test-Anwendung fÔøΩr Large Language Models (LLMs) zur Bewertung von Performance und Hardware-Anforderungen mit vergleichbaren und analysierbaren Ergebnissen.

## Architektur

### Hauptkomponenten

1. **Konfigurationsmanagement**
   - Laden von `config.json` mit Testparametern
   - Validierung der Konfiguration
   - Fehlerbehandlung bei fehlenden/falschen Werten

2. **Fragenmanagement**
   - Laden der Fragen aus `questions.json`
   - Sequenzielle Auswahl basierend auf `questions` Parameter
   - UnterstÔøΩtzung fÔøΩr parallele Anfragen (`concurrent`)

3. **LLM-Kommunikation**
   - OpenAI-kompatible API Anbindung
   - Asynchrone Anfragen mit aiohttp
   - Timeout-Handling
   - Connection Pooling fÔøΩr Performance

4. **Ergebnisverarbeitung**
   - Strukturierte JSON-Ausgabe im `results/` Ordner
   - Zeitstempel und Metadaten
   - Einzelergebnisse pro Frage
   - Aggregierte Statistiken

5. **Fehlerbehandlung & Logging**
   - Detailliertes Error-Logging
   - Graceful Degradation - kein Absturz
   - Logging in separate Datei
   - Verschiedene Log-Level (DEBUG, INFO, WARNING, ERROR)

## Datenstrukturen

### Input
- **config.json**: Testkonfiguration
- **questions.json**: Fragenkatalog
- **CLI-Parameter**: Ausgabedateiname

### Output (results/*.json)
```json
{
  "meta": {
    "start_date": "YYYY-MM-DD",
    "start_time": "HH:MM:SS.mmm",
    "end_date": "YYYY-MM-DD",
    "end_time": "HH:MM:SS.mmm",
    "server": "url aus config",
    "model": "model aus config",
    "concurrent": "concurrent aus config",
    "questions": "questions aus config",
    "timeout": "timeout aus config",
    "total_duration_ms": "berechnet"
  },
  "results": [
    {
      "question": "Frage aus questions.json",
      "answer": "Antwort vom LLM",
      "time": "Response Zeit in ms",
      "token": "Anzahl generierter Tokens",
      "quality": "GesamtqualitÔøΩtsbewertung (0-10)",
      "quality_metrics": {
        "structure": "Struktur-Bewertung (0-10)",
        "readability": "Lesbarkeits-Bewertung (0-10)",
        "completeness": "VollstÔøΩndigkeits-Bewertung (0-10)",
        "relevance": "Relevanz-Bewertung (0-10)",
        "factual_consistency": "Faktische Konsistenz (0-10)",
        "language_flow": "Sprachfluss-Bewertung (0-10)",
        "coherence": "KohÔøΩrenz-Bewertung (0-10)",
        "overall_quality": "GesamtqualitÔøΩt (0-10)"
      }
    }
  ],
  "aggregate": {
    "runtime_sum": "Summe aller Zeiten",
    "runtime_avg": "Durchschnitt",
    "runtime_min": "Minimum",
    "runtime_max": "Maximum",
    "token_sum": "Summe aller Tokens",
    "token_avg": "Durchschnitt",
    "token_min": "Minimum",
    "token_max": "Maximum",
    "quality_sum": "Summe aller Quality-Scores",
    "quality_avg": "Durchschnittliche QualitÔøΩt",
    "quality_min": "Minimale QualitÔøΩt",
    "quality_max": "Maximale QualitÔøΩt"
  }
}
```

## Implementierungsschritte

1. **Setup & Dependencies**
   - uv add openai (fÔøΩr OpenAI-kompatible API)
   - uv add aiohttp (fÔøΩr async HTTP)
   - uv add asyncio (fÔøΩr concurrent requests)
   - Logging-Konfiguration

2. **Core-Funktionen**
   - `load_config()`: Config laden und validieren
   - `load_questions()`: Fragen laden
   - `test_connection()`: Verbindung prÔøΩfen
   - `send_question()`: Einzelne Frage senden
   - `process_questions()`: Batch-Verarbeitung
   - `calculate_aggregates()`: Statistiken berechnen
   - `save_results()`: Ergebnisse speichern

3. **Error Handling**
   - Connection Errors
   - Timeout Errors
   - API Errors
   - File I/O Errors
   - JSON Parse Errors

4. **CLI Interface**
   ```bash
   python llm_stresstest.py <output_filename>
   # Beispiel: python llm_stresstest.py results_PC_FIN_qwen13b
   ```

## Robustheit-MaÔøΩnahmen

1. **Fehlertoleranz**
   - Try-catch fÔøΩr alle kritischen Operationen
   - Fallback-Werte bei Fehlern
   - Fortsetzung bei einzelnen fehlgeschlagenen Anfragen

2. **Logging**
   - Strukturiertes Logging mit Zeitstempeln
   - Separate Log-Datei pro Run
   - Error-Stack-Traces fÔøΩr Debugging

3. **Validierung**
   - Input-Validierung fÔøΩr Config und Questions
   - Type-Checking fÔøΩr API-Responses
   - Pfad-Validierung fÔøΩr Output-Dateien

4. **Performance**
   - Async/Await fÔøΩr parallele Anfragen
   - Connection Pooling
   - Timeout-Management

## Testing-Strategie

1. **Unit Tests**
   - Config-Loading
   - Question-Loading
   - Aggregation-Funktionen

2. **Integration Tests**
   - API-Kommunikation
   - File I/O
   - End-to-End Workflow

3. **Error Tests**
   - Verbindungsfehler
   - Timeout-Szenarien
   - Falsche Konfiguration

## Erweiterungen - Umgesetzt

### QualitÔøΩtsbewertung der Antworten ‚úÖ IMPLEMENTIERT
- **Quality Evaluator** vollstÔøΩndig in `llm_stresstest.py` integriert
- 8 QualitÔøΩtsmetriken: Struktur, Lesbarkeit, VollstÔøΩndigkeit, Relevanz, faktische Konsistenz, Sprachfluss, KohÔøΩrenz, GesamtqualitÔøΩt
- 4 zusÔøΩtzliche Kennzahlen: Wortanzahl, Satzanzahl, durchschnittliche SatzlÔøΩnge, Wiederholungsrate
- Automatische Bewertung bei jeder LLM-Antwort
- Quality-Aggregation in Statistiken (sum, avg, min, max)
- Erweiterte JSON-Struktur mit `quality` und `quality_metrics` Feldern

### Architektur-ÔøΩnderungen
- Quality Evaluator als integrierte Klasse statt separates Modul
- Neue AbhÔøΩngigkeiten (optional): spacy, sentence-transformers
- Erweiterte Datenstrukturen fÔøΩr QualitÔøΩtsmetriken
- Robuste Fehlerbehandlung bei Quality-Bewertung (Fallback auf 0.0)

### Dashboard & Auswertung ‚úÖ IMPLEMENTIERT
- **Streamlit Dashboard** (`llm_auswertung.py`) vollst√§ndig implementiert
- **5 Hauptbereiche**:
  - üìä √úbersicht: Gesamtstatistiken und Tabellenansicht aller Tests
  - üìù Log-Analyse: Durchsuchen und Filtern von Logs mit automatischer Fehleranzeige
  - ‚ö° Performance: Token/Zeit-Analyse, Performance-Rankings, Effizienz-Matrix
  - üîÑ Vergleiche: Server- und Modell-Vergleiche mit interaktiven Grafiken
  - üìà Qualit√§tsmetriken: Radar-Charts, Box-Plots, detaillierte Statistiken
- **Dark Mode Support** mit umfangreichen CSS-Optimierungen
- **Cross-Platform** Unterst√ºtzung (Windows, macOS, Linux)
- **Interaktive Visualisierungen** mit Plotly
- **CSV-Export** Funktionalit√§t
- **Live-Log-Monitoring** mit Fehler- und Warnungshervorhebung

### LLM Load Time Measurement ‚úÖ IMPLEMENTIERT
- **Warmup-Phase** zur Messung der LLM-Ladezeit
- **Cold Start Factor** Berechnung (Ladezeit/Durchschnittszeit)
- **Pr√§zise Zeitmessung** durch doppelte Ausf√ºhrung der ersten Frage
- **Erweiterte Metriken** in JSON-Output (`llm_load_time`, `cold_start_factor`)

### Server Naming ‚úÖ IMPLEMENTIERT
- **server_name** Konfigurationsfeld f√ºr sprechende Server-Bezeichnungen
- **Fallback-Mechanismus** auf URL wenn kein Name gesetzt
- **Dashboard-Integration** mit korrekter Anzeige der Server-Namen

### UI/UX Verbesserungen ‚úÖ IMPLEMENTIERT
- **Kontrastprobleme behoben**: Log-Anzeige mit hellem Hintergrund f√ºr bessere Lesbarkeit
- **Navigation optimiert**: Sidebar-Buttons mit korrekten Hover-Effekten in Light/Dark Mode
- **Elegante Animationen**: Smooth Transitions und Schatten-Effekte
- **Responsive Design**: Optimiert f√ºr verschiedene Bildschirmgr√∂√üen

### Globale Analyse-Features ‚úÖ IMPLEMENTIERT
- **Globale Performance-Analyse** in der √úbersichtsseite:
  - Aggregierte Performance-Metriken pro Modell √ºber alle Server hinweg
  - Balkendiagramm mit verbesserter Darstellung (mehrzeilige Labels)
  - Farbkodierung nach Server mit Legende f√ºr bessere √úbersicht
  - Performance-Ranking Tabelle mit Min/Max/Durchschnitt und Anzahl Tests
  - Gek√ºrzte Modell- und Server-Namen f√ºr bessere Lesbarkeit
- **Globale Qualit√§ts-Analyse** in der √úbersichtsseite:
  - Aggregierte Qualit√§ts-Metriken pro Modell √ºber alle Server hinweg
  - Balkendiagramm mit verbesserter Darstellung (mehrzeilige Labels)
  - Farbkodierung nach Server mit Legende
  - Qualit√§ts-Ranking Tabelle mit Min/Max/Durchschnitt
  - Hover-Details mit vollst√§ndigen Informationen
- **Visualisierungs-Verbesserungen**:
  - Mehrzeilige Labels (Modell + Server in separaten Zeilen)
  - Gek√ºrzte Namen f√ºr bessere Darstellung in Diagrammen
  - Entfernung der Performance-Verteilungs-Box-Plots (durch bessere globale Analyse ersetzt)

### Vergleichbarkeits-Features ‚úÖ IMPLEMENTIERT
- **Normalisierte Metriken** in get_dataframe() f√ºr faire Vergleiche:
  - `concurrent_efficiency`: Performance pro Thread (Performance √∑ concurrent)
  - `throughput_per_min`: Fragen pro Minute basierend auf durchschnittlicher Antwortzeit
  - `load_efficiency`: Anteil Netto-Inferenzzeit in Prozent (ohne LLM-Ladezeit)
  - `performance_normalized` und `quality_normalized`: Bestehende Metriken beibehalten
- **Erweiterte √úbersichtsseite**:
  - Vergleichbarkeits-Hinweis bei unterschiedlichen Konfigurationen
  - Tabelle mit `concurrent` und `throughput_per_min` Spalten
  - Fix f√ºr numpy.int64 Darstellung in Listen
- **Performance-Bereich √ºberarbeitet**:
  - Info-Box √ºber normalisierte Metriken
  - Neue Metrik-Tiles f√ºr alle normalisierten Werte
  - Scatter-Plot mit erweiterten Hover-Daten
- **Vergleichsbereich aktualisiert**:
  - Hinweis auf normalisierte Metriken in beiden Vergleichsmodi
  - Detailtabellen mit allen relevanten Spalten
  - Multi-Metrik Balkendiagramm nutzt normalisierte Werte
- **Plotly Dependency hinzugef√ºgt** f√ºr erweiterte Visualisierungen
- **Faire Vergleiche** zwischen Tests mit unterschiedlichen `questions` und `concurrent` Einstellungen

### Modell-Metadaten Integration ‚úÖ IMPLEMENTIERT
- **Automatischer Metadaten-Abruf** in `llm_stresstest.py`:
  - `get_model_metadata()` Methode f√ºr Ollama API-Abfrage
  - Abruf von `parameter_size`, `quantization_level`, `size_bytes`, `family`
  - Integration in `test_connection()` f√ºr fr√ºhen Metadaten-Abruf
  - Robuste Fallbacks f√ºr nicht-Ollama APIs mit graceful degradation
- **Erweiterte Datenstrukturen**:
  - Meta-Daten in JSON-Output mit Modell-Metadaten
  - Parser f√ºr verschiedene Parameter-Formate (B, M)
  - Error-Handling f√ºr API-Calls mit Debug-Logging
- **Dashboard-Erweiterungen** in `llm_auswertung.py`:
  - Erweiterte DataFrame-Spalten: `parameter_size`, `quantization_level`, `size_gb`
  - Neue Effizienz-Metrik: `performance_per_billion_params`
  - Modellgr√∂√üe in GB f√ºr bessere Lesbarkeit
  - Erweiterte √úbersichtstabelle mit Modell-Metadaten
- **Neue Effizienz-Analyse-Sektion**:
  - Scatter-Plot: Performance pro Parameter vs. Parameter-Anzahl
  - Farbkodierung nach Quantisierung, Gr√∂√üe als Bubble-Size
  - Quantisierungs-Vergleich mit Performance-Balkendiagramm
  - Conditional display basierend auf verf√ºgbaren Metadaten
- **Robuste Implementierung**:
  - Graceful degradation bei fehlenden Metadaten
  - Logging f√ºr erfolgreiche Metadaten-Abrufe
  - Performance-Berechnung pro Milliarde Parameter

### Dashboard-Optimierungen - Phase 3 ‚úÖ IMPLEMENTIERT
- **Performance-Analyse √ºberarbeitet**:
  - Doppelte "Performance-Details" Sektion entfernt f√ºr klarere Struktur
  - Neue Gliederung: Performance-Ranking, LLM Load Time Analyse, Performance-Empfehlungen, Effizienz-Matrix
  - Strukturierte Performance-Empfehlungen basierend auf Hardware-Optimierung
- **Zeitverteilung nach Modell erweitert**:
  - Flexible Visualisierungs-Optionen mit interaktiver Auswahl
  - Balkendiagramm (Durchschnitt), Violin-Plot, Histogram, Statistik-Tabelle
  - Detaillierte Zeitstatistiken f√ºr umfassende Analyse
- **√úbersicht-Sektion verbessert**:
  - Effizienz-Analyse durch Performance-Kuchendiagramme ersetzt
  - Server-Performance Anteil und Modell-Performance Anteil als Pie-Charts
  - Bessere √úbersichtlichkeit und intuitivere Darstellung

### Bugfixes - Dashboard ‚úÖ IMPLEMENTIERT
- **NoneType-Fehler behoben** in `llm_auswertung.py`:
  - TypeError bei `size_bytes is None` in Zeile 527 der `get_dataframe()` Methode
  - Zus√§tzliche None-Check: `if row['size_bytes'] is not None and row['size_bytes'] > 0:`
  - Robuste Behandlung von fehlenden Modell-Metadaten in JSON-Dateien
  - Fehler verhinderte Dashboard-Start bei Dateien ohne vollst√§ndige Metadaten

### Dashboard UX-Verbesserungen ‚úÖ IMPLEMENTIERT
- **Interaktive Info-Features** f√ºr bessere Benutzerf√ºhrung:
  - Info-Kreise (‚ÑπÔ∏è) bei speziellen Features (Reasoning, Multimodal, Tool-Support)
  - Navigation von Info-Buttons zur Modell-Information Seite
  - Detaillierte Feature-Definitionen auf der Modell-Information Seite
  - Automatisches Ausklappen der entsprechenden Info-Bereiche basierend auf URL-Parameter
- **Session State Management**:
  - Saubere Navigation zwischen Dashboard-Bereichen
  - Persistenz von erweiterten Info-Bereichen w√§hrend der Session
  - Intuitive Benutzerf√ºhrung zu relevanten Informationen
- **Feature-Dokumentation erweitert**:
  - Umfassende Erkl√§rungen f√ºr Reasoning (CoT, ToT, ReAct, Self-Reflection)
  - Multimodale Capabilities (Vision, Audio, Video, Text-to-Speech)
  - Tool-Support Definitionen (Function Calling, Code Execution, Web Search)

## Erweiterungsm√∂glichkeiten - Noch offen

- A/B Testing zwischen verschiedenen Modellen
- Benchmark-Vergleiche mit Standarddatens√§tzen
- Integration mit CI/CD Pipelines
- REST API f√ºr externe Tools
- Automatische Report-Generierung
- E-Mail-Benachrichtigungen bei Tests